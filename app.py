import os
import streamlit as st
from st_supabase_connection import SupabaseConnection

from langchain_community.vectorstores.supabase import SupabaseVectorStore
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# Load secrets
SUPABASE_URL = st.secrets["SUPABASE_URL"]
SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# Initialize Supabase via Streamlit connector
supabase_conn = st.connection(
    "supabase",
    type=SupabaseConnection,
    url=SUPABASE_URL,
    key=SUPABASE_KEY,
)

# Setup embeddings + vector store
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=OPENAI_API_KEY,
)
vectorstore = SupabaseVectorStore(
    client=supabase_conn.client,
    embedding=embeddings,
    table_name="documents",
    query_name="match_documents",
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Build custom prompt template
system_prompt = """‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶π‡¶ï‡¶æ‡¶∞‡ßÄ‡•§
‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶¶‡ßá‡¶ñ‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡ßá‡¶¨‡ßá‚Äî‡¶Ø‡¶¶‡¶ø ‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶¶‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡ßá ‡¶¨‡¶≤‡ßã '‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡ßá‡¶á'‡•§
‡¶§‡¶•‡ßç‡¶Ø‡¶∏‡ßÇ‡¶§‡ßç‡¶∞:
{context}
"""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}")
    ]
)
# Initialize LLM
llm = ChatOpenAI(model_name="gpt-4.1-mini", temperature=0.5)

# Create chains
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)



# Streamlit UI
st.title("üìö ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ HSC26 RAG ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï")
query = st.text_area("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®:", height=100)

if st.button("‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶™‡¶æ‡¶®"):
    with st.spinner("‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡¶ö‡ßç‡¶õ‡ßá‚Ä¶"):
        res = rag_chain.invoke({"input": query})
        answer = res.get("output") or res.get("result") or "‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡ßá‡¶á"
        st.success(answer)

        st.markdown("#### ‡¶§‡¶•‡ßç‡¶Ø‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®:")
        for idx, doc in enumerate(res.get("source_documents", []), 1):
            st.write(f"Chunk {idx}: {doc.page_content[:200]}‚Ä¶")
