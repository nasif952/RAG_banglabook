import streamlit as st
from st_supabase_connection import SupabaseConnection
from langchain_community.vectorstores.supabase import SupabaseVectorStore
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import time

# Load secrets
SUPABASE_URL = st.secrets["SUPABASE_URL"]
SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

st.set_page_config(page_title="ржмрж╛ржВрж▓рж╛ HSC26 RAG рж╕рж╣рж╛ржпрж╝ржХ", page_icon="ЁЯУЪ")
st.title("ЁЯУЪ ржмрж╛ржВрж▓рж╛ HSC26 RAG рж╕рж╣рж╛ржпрж╝ржХ")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

# Connection checks
@st.cache_resource
def check_connections():
    def check_supabase():
        try:
            sup = st.connection("supabase", type=SupabaseConnection,
                                url=SUPABASE_URL, key=SUPABASE_KEY)
            _ = sup.client.auth.get_user()
            return True, "тЬЕ Supabase Connected"
        except Exception as e:
            return False, f"тЭМ Supabase Error: {e}"

    def check_openai():
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            client.models.list()
            return True, "тЬЕ OpenAI Connected"
        except Exception as e:
            return False, f"тЭМ OpenAI Error: {e}"
    
    return check_supabase(), check_openai()

# Setup RAG chain
@st.cache_resource
def setup_rag_chain():
    try:
        supabase_conn = st.connection("supabase", type=SupabaseConnection,
                                      url=SUPABASE_URL, key=SUPABASE_KEY)
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)
        vectorstore = SupabaseVectorStore(client=supabase_conn.client, embedding=embeddings,
                                          table_name="documents", query_name="match_documents")
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        
        system_prompt = """рждрзБржорж┐ ржПржХржЬржи рж╕рж╣рж╛ржпрж╝ржХ ржмрж╛ржВрж▓рж╛ рж╕рж╣ржХрж╛рж░рзАред
рждрзБржорж┐ рж╢рзБржзрзБржорж╛рждрзНрж░ ржирж┐ржЪрзЗрж░ рждржерзНржпрж╕рзВрждрзНрж░ ржжрзЗржЦрзЗ ржЙрждрзНрждрж░ ржжрзЗржмрзЗтАФржпржжрж┐ рждржерзНржп ржирж╛ ржерж╛ржХрзЗ, ржжржпрж╝рж╛ ржХрж░рзЗ ржмрж▓рзЛ 'рждржерзНржп ржирзЗржЗ'ред

ржкрзВрж░рзНржмржмрж░рзНрждрзА ржХржерзЛржкржХржержирзЗрж░ ржкрзНрж░рж╕ржЩрзНржЧ ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рзЗ ржЙрждрзНрждрж░ ржжрж╛ржУред

рждржерзНржпрж╕рзВрждрзНрж░:
{context}
"""
        
        prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5, api_key=OPENAI_API_KEY)
        qa_chain = create_stuff_documents_chain(llm, prompt)
        rag_chain = create_retrieval_chain(retriever, qa_chain)
        
        return rag_chain, None
    except Exception as e:
        return None, str(e)

# Check connections
(sup_status, sup_msg), (open_status, open_msg) = check_connections()

# Sidebar
with st.sidebar:
    st.markdown("### ЁЯФЧ Connection Status")
    st.write(sup_msg)
    st.write(open_msg)
    
    st.markdown("### ЁЯТм Chat Controls")
    st.write(f"ЁЯУЭ Messages: {len(st.session_state.messages)}")
    
    if st.button("ЁЯЧСя╕П Clear Chat History"):
        st.session_state.messages = []
        st.success("Chat cleared!")
        time.sleep(0.5)
        st.rerun()
    
    # Export chat option
    if len(st.session_state.messages) > 0:
        chat_export = ""
        for msg in st.session_state.messages:
            role = "ЁЯСд ржЖржкржирж┐" if msg["role"] == "user" else "ЁЯдЦ рж╕рж╣рж╛ржпрж╝ржХ"
            chat_export += f"{role}: {msg['content']}\n\n"
        
        st.download_button(
            "ЁЯТ╛ Export Chat",
            data=chat_export,
            file_name=f"chat_history_{int(time.time())}.txt",
            mime="text/plain"
        )

# Check if connections are working
if not (sup_status and open_status):
    st.error("тЭМ Connection failed. Please check your configuration.")
    st.stop()

# Setup RAG chain if not already done
if st.session_state.rag_chain is None:
    with st.spinner("Setting up RAG system..."):
        rag_chain, error = setup_rag_chain()
        if rag_chain:
            st.session_state.rag_chain = rag_chain
            st.success("тЬЕ RAG system ready!")
        else:
            st.error(f"тЭМ RAG setup failed: {error}")
            st.stop()

# Function to get conversation context
def get_conversation_context():
    if len(st.session_state.messages) == 0:
        return ""
    
    # Get last 4 messages for context (2 exchanges)
    recent_messages = st.session_state.messages[-4:]
    context = "\n\nрж╕рж╛ржорзНржкрзНрж░рждрж┐ржХ ржХржерзЛржкржХржержи:\n"
    
    for msg in recent_messages:
        role = "ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзА" if msg["role"] == "user" else "рж╕рж╣рж╛ржпрж╝ржХ"
        context += f"{role}: {msg['content']}\n"
    
    return context

# Display chat messages
st.markdown("### ЁЯТм ржХржерзЛржкржХржержи")

# Create a container for chat messages
chat_container = st.container()

with chat_container:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            # Show sources for assistant messages
            if message["role"] == "assistant" and "sources" in message:
                with st.expander("ЁЯУЪ рждржерзНржпрж╕рзВрждрзНрж░"):
                    for i, source in enumerate(message["sources"], 1):
                        st.markdown(f"**Source {i}:** {source}")

# Chat input
if prompt := st.chat_input("ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржи рж▓рж┐ржЦрзБржи..."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generate assistant response
    with st.chat_message("assistant"):
        with st.spinner("ржЪрж┐ржирзНрждрж╛ ржХрж░ржЫрж┐..."):
            try:
                # Get conversation context
                conversation_context = get_conversation_context()
                
                # Create enhanced input with context
                enhanced_input = f"{prompt}{conversation_context}"
                
                # Get response from RAG
                result = st.session_state.rag_chain.invoke({"input": enhanced_input})
                response = result.get("answer", "рждржерзНржп ржирзЗржЗ")
                
                # Extract sources
                sources = []
                if "context" in result:
                    sources = [doc.page_content[:200] + "..." for doc in result["context"]]
                
                # Display response
                st.markdown(response)
                
                # Add assistant response to chat history
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "sources": sources
                })
                
            except Exception as e:
                error_msg = f"ржжрзБржГржЦрж┐ржд, ржПржХржЯрж┐ рж╕ржорж╕рзНржпрж╛ рж╣ржпрж╝рзЗржЫрзЗ: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": error_msg
                })

# Quick action buttons
if len(st.session_state.messages) == 0:
    st.markdown("### ЁЯЪА ржжрзНрж░рзБржд рж╢рзБрж░рзБ ржХрж░рзБржи")
    st.markdown("ржирж┐ржЪрзЗрж░ ржмрж┐рж╖ржпрж╝ржЧрзБрж▓рзЛ рж╕ржорзНржкрж░рзНржХрзЗ ржЬрж┐ржЬрзНржЮрж╛рж╕рж╛ ржХрж░рждрзЗ ржкрж╛рж░рзЗржи:")
    
    col1, col2, col3, col4 = st.columns(4)
    
    quick_questions = [
        ("ЁЯУЦ", "ржмрж╛ржВрж▓рж╛ рж╕рж╛рж╣рж┐рждрзНржп", "ржмрж╛ржВрж▓рж╛ рж╕рж╛рж╣рж┐рждрзНржпрзЗрж░ ржЗрждрж┐рж╣рж╛рж╕ рж╕ржорзНржкрж░рзНржХрзЗ ржмрж▓рзБржи"),
        ("ЁЯФм", "ржмрж┐ржЬрзНржЮрж╛ржи", "ржкржжрж╛рж░рзНржержмрж┐ржЬрзНржЮрж╛ржирзЗрж░ ржорзВрж▓ ржирзАрждрж┐ржЧрзБрж▓рзЛ ржХрзА?"),
        ("ЁЯУК", "ржЧржгрж┐ржд", "ржХрзНржпрж╛рж▓ржХрзБрж▓рж╛рж╕рзЗрж░ ржорзВрж▓ ржзрж╛рж░ржгрж╛ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рзБржи"),
        ("ЁЯМН", "ржнрзВржЧрзЛрж▓", "ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ ржнрзМржЧрзЛрж▓рж┐ржХ ржЕржмрж╕рзНржерж╛ржи рж╕ржорзНржкрж░рзНржХрзЗ ржмрж▓рзБржи")
    ]
    
    cols = [col1, col2, col3, col4]
    for i, (emoji, title, question) in enumerate(quick_questions):
        with cols[i]:
            if st.button(f"{emoji} {title}"):
                # Add the question as if user typed it
                st.session_state.messages.append({"role": "user", "content": question})
                st.rerun()

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; font-size: 12px;'>
    ЁЯЪА Powered by LangChain, Supabase, OpenAI, and Streamlit<br>
    ЁЯТб Tip: ржЖржкржирж┐ ржзрж╛рж░рж╛ржмрж╛рж╣рж┐ржХ ржкрзНрж░рж╢рзНржи ржХрж░рждрзЗ ржкрж╛рж░рзЗржи, рж╕рж┐рж╕рзНржЯрзЗржо ржкрзВрж░рзНржмржмрж░рзНрждрзА ржХржерзЛржкржХржержи ржоржирзЗ рж░рж╛ржЦржмрзЗ
</div>
""", unsafe_allow_html=True)
